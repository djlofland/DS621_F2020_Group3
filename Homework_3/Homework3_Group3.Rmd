---
title: 'DS 621 Fall2020: Homework 3 (Group3)'
subtitle: 'Crime Logistic Regression'
author: 'Zach Alexander, Sam Bellows, Donny Lofland, Joshua Registe, Neil Shah, Aaron Zalki'
data: '09/26/2020'
output:
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
---

Source code: [https://github.com/djlofland/DS621_F2020_Group3/tree/master/Homework_3](https://github.com/djlofland/DS621_F2020_Group3/tree/master/Homework_3)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(MASS)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(forecast)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(mlbench)
library(ggcorrplot)
library(DataExplorer)
library(timeDate)
library(caret)
library(GGally)
library(corrplot)
library(RColorBrewer)
library(tibble)
library(tidyr)
library(tidyverse)
library(dplyr)
library(reshape2)
library(mixtools)
library(tidymodels)
library(ggpmisc)
library(regclass)

#' Print a side-by-side Histogram and QQPlot of Residuals
#'
#' @param model A model
#' @examples
#' residPlot(myModel)
#' @return null
#' @export
residPlot <- function(model) {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
  plot(residuals(model))
  hist(model[["residuals"]], freq = FALSE, breaks = "fd", main = "Residual Histogram",
       xlab = "Residuals",col="lightgreen")
  lines(density(model[["residuals"]], kernel = "ep"),col="blue", lwd=3)
  curve(dnorm(x,mean=mean(model[["residuals"]]), sd=sd(model[["residuals"]])), col="red", lwd=3, lty="dotted", add=T)
  qqnorm(model[["residuals"]], main = "Residual Q-Q plot")
  qqline(model[["residuals"]],col="red", lwd=3, lty="dotted")
  par(mfrow = c(1, 1))
}

#' Print a Variable Importance Plot for the provided model
#'
#' @param model The model
#' @param chart_title The Title to show on the plot
#' @examples
#' variableImportancePlot(myLinearModel, 'My Title)
#' @return null
#' @export
variableImportancePlot <- function(model=NULL, chart_title='Variable Importance Plot') {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  # use caret and gglot to print a variable importance plot
  varImp(model) %>% as.data.frame() %>% 
    ggplot(aes(x = reorder(rownames(.), desc(Overall)), y = Overall)) +
    geom_col(aes(fill = Overall)) +
    theme(panel.background = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    scale_fill_gradient() +
    labs(title = chart_title,
         x = "Parameter",
         y = "Relative Importance")
}


#' Print a Facet Chart of histograms
#'
#' @param df Dataset
#' @param box Facet size (rows)
#' @examples
#' histbox(my_df, 3)
#' @return null
#' @export
histbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]]
    
    for (i in seq_along(ndf)) {
            data <- na.omit(unlist(df[, i]))
            hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
                 xlab = ndf[i], freq = FALSE)
            lines(density(data, kernel = "ep"), col = 'red')
    }
    
    par(mfrow = c(1, 1))
}

#' Extract key performance results from a model
#'
#' @param model A linear model of interest
#' @examples
#' model_performance_extraction(my_model)
#' @return data.frame
#' @export
model_performance_extraction <- function(model=NULL) {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  data.frame("RSE" = model$sigma,
             "Adj R2" = model$adj.r.squared,
             "F-Statistic" = model$fstatistic[1])
}

```

## Instructions

### Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

* zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
* indus: proportion of non-retail business acres per suburb (predictor variable)
* chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
* nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)
* rm: average number of rooms per dwelling (predictor variable)
* age: proportion of owner-occupied units built prior to 1940 (predictor variable)
* dis: weighted mean of distances to five Boston employment centers (predictor variable)
* rad: index of accessibility to radial highways (predictor variable)
* tax: full-value property-tax rate per $10,000 (predictor variable)
* ptratio: pupil-teacher ratio by town (predictor variable)
* black: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
* lstat: lower status of the population (percent) (predictor variable)
* medv: median value of owner-occupied homes in $1000s (predictor variable)
* target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

### Deliverables

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned prediction (probabilities, classifications) for the evaluation data set. Use 0.5 threshold.
* Include your R statistical programming code in an Appendix.

## Introduction

[TODO - Insert overview test describing the data and how we will generally approach the problem]

## 1. Data Exploration

*Describe the size and the variables in the crime training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you arenâ€™t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. You should have your own thoughts on what to tell the boss. These are just ideas.*

### Dataset

[TODO - Describe data set, # columns, target name, row count - anything special it's helpful to know before going into analysis]

There are two files provided:

* **crime-training-data_modified.csv** - hold data from training a model
* **crime-evaluation-data_modified.csv** - holdout data used to for evaluation.  Note this dataset doesn't include the target column so while we can predict classes, our model will have to be scored by an outside group with knowledge of the actual classes.

```{r load_data, echo=FALSE}
# Load Moneyball baseball dataset
df <- read.csv('datasets/crime-training-data_modified.csv')
df_eval <- read.csv('datasets/crime-evaluation-data_modified.csv')
```

Below, we created a chart that describes each variable in the dataset and the theoretical effect it will have on the crime rate.

[TODO - put image here]

### Summary Stats

We compiled summary statistics on our dataset to better understand the data before modeling. 

```{r columns, echo=FALSE}
# Display summary statistics
summary(df)
```

The first observation is that we have no missing data (coded as NA's).

[TODO - Insert commentary based on initial summary stats]

### Check Class Bias

We have two target values, 0 and 1.  When building models, we ideally want an equal representation of both classes.  As class imbalance deviates, our model performance will suffer both form effects of differential variance between the classes and bias towards picking the more represented class.  For logistic regression, if we see strong imbalance, we can 1) up-sample the smaller group, down-sample the larger group, or adjust or threshold for assigning the predicted value away from 0.5.

```{r}
#[TODO - show class counts]
```

[TODO - any commentary on class (im)balance]

### Distributions

Next, we visualize the distribution profiles for each of the predictor variables. This will help us to make a plan on which variable to include, how they might be related to each other or the target, and finally identify outliers or transformations that might help improve model resolution.

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% 
  gather(key = 'variable', value = 'value')

# Histogram plots of each variable
ggplot(gather_df) + 
  geom_histogram(aes(x=value, y = ..density..), bins=30) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=3)
```

The distribution profiles show the prevalence of kurtosis, specifically right skew in variables [TODO - insert right variable names] and left skew in [TODO - insert left skew variable names]. These deviations from a traditional normal distribution can be problematic for linear regression assumptions, and thus we might need to transform the data.  Several features appear to be binary [TODO - list and explain].  Furthermore [TODO - insert bimodel column names] appear bimodal. Bimodal features in a dataset are both problematic and interesting and potentially an area of opportunity and exploration.  Bimodal data suggests that there are possibly two different groups or classes within the feature.

Two possibilities immediately come to mind.  The bimodal nature could be caused by ...[TODO commentary]

While we don't tackle feature engineering in this analysis, if we were performing a more in-depth analysis, we could leverage the package, `mixtools` (see R Vignette).  This package helps regress *mixed models* where data can be subdivided into subgroups.  

[TODO - maybe drop this section, not sure if needed]

Here is a quick example showing a possible mix within `indis`:

```{r, echo=FALSE}
# Select indus column and remove any missing data
#df_mix <- df %>% 
#  dplyr::select(indus)
#
# Calculate mixed distributions for indus
#indus_so_mix <- normalmixEM(df_mix$indus, 
#                            lambda = .5, 
#                            mu = c(400, 1200), 
#                            sigma = 5, 
#                            maxit=60)

# Simple plot to illustrate possible bimodal mix of groups
#plot(indus_so_mix, 
#     whichplots = 2,
#     density = TRUE, 
#     main2 = "`indus` Possible Distributions", 
#     xlab2 = "indus")
```

Lastly, several features have both a distribution along with a high number of values at an extreme.  We should first vet whether those are reasonable values and if so, possible create a new feature separating out those values and only transform the existing non-extreme values [TODO - this needs way better explanation and only include if we end up doing this]

### Boxplots

In addition to creating histogram distributions, we also elected to use box-plots to get an idea of the spread of each variable. 

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% 
  gather(key = 'variable', value = 'value')

# Boxplots for each variable
ggplot(gather_df, aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(. ~variable, scales='free', ncol=6)
```

The box-plots reveal significant outliers that may need to be imputed if necessary.

### Variable Plots

Finally, we wanted to plot scatter plots of each variable versus the target variable, TARGET_WINS, to get an idea of the relationship between them. 

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Plot scatter plots of each variable versus the target variable
featurePlot(df[,1:ncol(df)-1], df[,ncol(df)], pch = 20)
```

[TODO - insert commentary]

### Missing Data

Fortunately, we see no NAs, or missing data in the provided training dataset.

```{r echo=FALSE} 
# Identify missing data by Feature and display percent breakout
missing <- colSums(df %>% sapply(is.na))
missing_pct <- round(missing / nrow(df) * 100, 2)
stack(sort(missing_pct, decreasing = TRUE))
```

### Handle Outliers

```{r echo=FALSE}
# For continuous features, we can use mean or median to impute and replace outliers; however, with categorical features, a KNN strategy may be more appropriate where we replace unusual values with values from other similar properties.

no_outlier_df <- df

# [TODO - Handle continuous variables]


# [TODO - handle categorical variables]

# note: clean_df is post-outlier cleanup
clean_df <- no_outlier_df
```

### Feature-Target Correlations

With our outliers data imputed correctly, we can now build plots to quantify the correlations between our target variable and predictor variable. We will want to choose those with stronger positive or negative correlations.  Features with correlations closer to zero will probably not provide any meaningful information on explaining crime patterns.

```{r echo=FALSE}
# Show feature correlations/target by decreasing correlation
stack(sort(cor(clean_df[,1], clean_df[,2:ncol(clean_df)])[,], decreasing=TRUE))
```

[TODO - insert variable names] have the highest correlation (positive) with target; this makes sense given that more [TODO] means more [TODO] The other variables have weak or slightly negative correlation, which implies they have less predictive power.

### Multicolinearity

One problem that can occur with multi-variable regression is correlation between variables, or multicolinearity. A quick check is to run correlations between variables.   

```{r echo=FALSE}
# Calculate and plot the Multicolinearity
correlation = cor(clean_df, use = 'pairwise.complete.obs')

corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))
```

We can see that some variables are highly correlated with one another, such as [TODO - insert variable names]. When we start considering features for our models, we'll need to account for the correlations between features and avoid including pairs with strong correlations.

As a note, this dataset is challenging as many of the predictive features go hand-in-hand with other feature and multicolinearity will be a problem.  [TODO why?] 

## 2. Data Preparation

To summarize our data preparation and exploration, we can distinguish our findings into a few categories below:

### Removed Fields

[TODO insert commentary]  

### Missing Values

[TODO insert commentary]  

### Outliers

[TODO insert commentary]  

### Transform non-normal variables

Finally, as mentioned earlier in our data exploration, and our findings from our histogram plots, we can see that some of our variables are highly skewed. To address this, we decided to perform some transformations to make them more normally distributed. Here are some plots to demonstrate the changes in distributions before and after the transformations:  

```{r echo=FALSE, fig.height=12, fig.width=10, message=FALSE, warning=FALSE}

# created empty data frame to store transformed variables
df_temp <- data.frame(matrix(ncol = 1, nrow = length(clean_df$target)))

# performed boxcox transformation after identifying proper lambda
# df_temp$BATTING_3B <- clean_df$BATTING_3B
# batting3b_lambda <- BoxCox.lambda(clean_df$BATTING_3B)
# df_temp$BATTING_3B_transform <- log(clean_df$BATTING_3B)

# [TODO - do other columns]

#df_temp <- df_temp[, 2:13]

#histbox(df_temp, c(6, 2))
```

### Finalizing the dataset for model building 

With our transformations complete, we can now add these into our `clean_df` dataframe and continue on to build our models.

```{r}
# Build clean dataframe with transformation

# [TODO - update all this code]
clean_df <- data.frame(cbind(clean_df 
#                        BATTING_3B_transform = df_temp$BATTING_3B_transform,
#                        BATTING_HR_transform = df_temp$BATTING_HR_transform,
#                        BASERUN_SB_transform = df_temp$BASERUN_SB_transform,
#                        PITCHING_BB_transform = df_temp$PITCHING_BB_transform,
#                        PITCHING_SO_transform = df_temp$PITCHING_SO_transform,
#                        FIELDING_E_transform = df_temp$FIELDING_E_transform
                        ))

is.na(clean_df) <- sapply(clean_df, is.infinite)

# Impute missing value with the mean
#mean = mean(clean_df$BATTING_3B_transform, na.rm = TRUE)
#mean2 = mean(clean_df$BASERUN_SB_transform, na.rm = TRUE)
#mean3 = mean(clean_df$PITCHING_SO_transform, na.rm = TRUE)

# clean_df$BATTING_3B_transform[is.na(clean_df$BATTING_3B_transform)] <- mean
# clean_df$BASERUN_SB_transform[is.na(clean_df$BASERUN_SB_transform)] <- mean2
# clean_df$PITCHING_SO_transform[is.na(clean_df$PITCHING_SO_transform)] <- mean3
```


## 3. Build Models

*Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.* 
*Be sure to explain how you can make inferences from the model, as well as discuss other relevant model output. Discuss the coefficients in the models, do they make sense? Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.* 

### Model-building methodology  

With a solid understanding of our dataset at this point, and with our data cleaned, we can now start to build out some of our binary logistic regression models.  

First, we decided to split our cleaned dataset into a training and testing set (80% training, 20% testing). This was necessary as the provided holdout evaluation dataset doesn't provide target values so we cannot measure our model performance against that dataset.  Using our training dataset, we decided to run a binary logistic regression model (Model #1) that included all non-transformed features that we hadn't removed following our data cleaning process mentioned above. Next, in Model #2, to help us select the optimal set of features, we ran the `stepAIC` function in R to perform step-wise selection on our initial model to help reduce the number of features and address some of our multicolinearity issues from Model #1 (explained more below). Finally, our last model (Model #3) included all data from our cleaned dataset, but also contained some of our transformed features in replacement of their non-transformed counterparts. Additionally, we ran this model through step-wise selection as well to determine the optimal set of features and to reduce multicolinearity as much as possible.  

### Examining our model coefficients  

Throughout our model-building process, we noticed that many of our model outputs yielded a few coefficient values that seemed to contradict our initial estimates. For instance, in Model #1:  

**Negative values for coefficients that we'd expect to be positive**

+ [TODO - add examples if present]  

**Positive values for coefficients that we'd expect to be negative**  

+ [TODO - add examples if present]  

This is a trend we saw throughout the three models that we built, although Model #3 was able to adjust for this better than our first two models -- we can likely attribute this phenomenon to multicolinearity. Since we noticed in our initial data exploration that many variables in the dataset were highly correlated with one another (i.e. [TODO - give examples]), this phenomenon likely is increasing the variance of the coefficient estimates, making them difficult to interpret (and in some cases such as the features listed above, they are switching the signs). This was also supported by our Variance Inflation Factor (VIF) tests, which showed high values for features such as [TODO - insert examples]. In our final model (Model #3), we made sure to keep this in mind in order to get a better handle on our coefficients and reduce multicolinearity -- mainly, we removed certain variables that had high VIF scores through our stepwise selection process. Later in our discussion, we'll discuss whether or not we'd keep our final model based on this issue.  

### Model 1  

Below is our initial multiple linear regression model that includes all features from our cleaned dataset.

```{r echo=FALSE}
# Splitting the dataset into train/testing set with 80/20 split
set.seed(3)
df_split <- initial_split(clean_df, prop = 0.8)
df_train <- training(df_split)
df_test <- testing(df_split)

# Model 1 - Include all features and leverage stepAIC to help identify ideal features

# [TODO - add column names]
multi_lm <- glm(target ~ ., data=df_train)
(lm_s <- summary(multi_lm))
confint(multi_lm)
#residPlot(lm_s)
```

We can explicitly call out our variable importance using the `caret` package which will determine our feature importance for each of the binary logistic regression models individually.

We also examine the VIF of our variables to check for multicolinearity in the model.

```{r echo=FALSE}
# Display Variable feature importance plot
variableImportancePlot(multi_lm, "Model 1 LM Variable Importance")

# print variable inflation factor score
print('VIF scores of predictors')

VIF(multi_lm)
```

### Model 2  

In our second model, below, we decided to use stepwise selection in order to help us determine the optimal set of features from our original, cleaned dataset.

```{r echo=FALSE}
# Build model 2 - this is Model 1 with only significant features (using stepAIC)
mult_lm_final <- stepAIC(multi_lm, direction = "both",
                         scope = list(upper = multi_lm, lower = ~ 1),
                         scale = 0, trace = FALSE)
# Display Model 2 Summary
(lmf_s <- summary(mult_lm_final))

# Display Model 2 REsidual plots
#residPlot(lmf_s)
```



```{r echo=FALSE}
# Display Variable feature importance plot
variableImportancePlot(mult_lm_final, "Model 2 LM Variable Importance")

# print variable inflation factor score
print('VIF scores of predictors')

VIF(mult_lm_final)
```

### Model 3

In our third model, we decided to utilize some of our transformed variables to compare against our initial model. Additionally, similar to model 2, we used step-wise selection to determine feature importance and select the simplest model possible.

```{r echo=FALSE}

# Model 3 - Build linear model
# [TODO - add column names]
multi_lm_2 <- glm(target ~ ., data=df_train)

# Model 3 - Show model summary info
(lm_s_2 <- summary(multi_lm_2))

# Model 3 - Show Confidence and Residual Plots
confint(multi_lm_2)
#residPlot(lm_s_2)

# Model 3 - use stepAIC to select significant features
mult_lm_final_2 <- stepAIC(multi_lm_2, direction = "both",
                         scope = list(upper = multi_lm_2, lower = ~ 1),
                         scale = 0, trace = FALSE)

# Model 3 - Show summary of revised model with only significant features included
(lmf_s_2 <- summary(mult_lm_final_2))

# model 3 - Show residual plots
#residPlot(lmf_s_2)
```

We can assess variable importance of our model with our original dataset (Model 2) to our transformed dataset (Model 3) as shown below. We notice that [TODO - add feature names] are much closer together in terms of weight in predicting target_wins than in Model 1 (pre-transformed)

```{r echo=FALSE}
# Model 3 - Show feature importance in final model after stepAIC
variableImportancePlot(mult_lm_final_2, "Model 3 LM Variable Importance")

# Model 3 - show Variance Inflation Factor score
print('VIF scores of predictors')

VIF(mult_lm_final_2)
```

## 4. Model Selection & Analysis 

*For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.*

The following table discusses each of the model performance metrics on the training dataset. These values indicate there is a minor improvement in model performance after applying transformations and selecting for significant parameters.  

All models achieved a positive adjusted R^2 pointing to a weakly-positive relationship between the target variable TOTAL_WINs and the model predictor variables. 

From the previous section and residual diagnostics (both the residual histogram and QQ plot)--all three models have normally distributed residuals with mean 0. There appears to be no heteroskedasticity (non-constant variance) or serial-correlation within the residuals--thus strictly from a linear regression perspective, none of the assumptions are violated and thus we can make inferential predictions based on our model. 

Next looking at the F-statistic for all models were large and achieved a statistically significant p-value than than a default alpha of .05; thus for all three models the Null hypothesis that is rejected that 0s for our regression coefficients would fit the data better. 

From the exploratory data analysis we saw that many of the predictor variables were correlated with each other--this makes sense given that baseball is a team sport, and thus different position scoring/runs helps contribute to wins. However this could be an issue from a multicolinearity, that can lead to unstable regression fits. From the previous section we used VIF (Variable Inflation Factor) to gauge better models by preferring VIF values less than 5. 


```{r}

# Build summary table with each model performance metrics
SummaryTable <- bind_rows(
  model_performance_extraction(lm_s),
  model_performance_extraction(lmf_s),
  model_performance_extraction(lmf_s_2)
) 

# Add row names
#rownames(SummaryTable) <- c("Model 1", "Model 2", "Model 3")

# Display nice format table
SummaryTable %>% 
  kable() %>% 
  kable_styling(
    bootstrap_options = c("hover", "condensed", "responsive"),
    full_width = F)
```

The following figure represents the performance of 3 of the models on both the testing and on the training dataset. for this multiple linear regression problem, the adjusted R2 value was used as the metric for performance on predicting the target wins.

```{r, message = F}
# Build prediction results dataset for plotting (each model with training and test data)
results <- multi_lm %>%
  predict(df_train) %>% 
  as.data.frame() %>% 
  mutate(Predicted = df_train$TARGET_WINS, model = "Model 1") %>% 
  bind_rows(mult_lm_final %>% 
              predict(df_train) %>% 
              as.data.frame() %>% 
              mutate(Predicted = df_train$TARGET_WINS, model = "Model 2"),
            mult_lm_final_2 %>% 
              predict(df_train) %>% 
              as.data.frame() %>% 
              mutate(Predicted = df_train$TARGET_WINS, model = "Model 3")) %>% 
  rename("Observed" = ".") %>% 
  mutate(dataset = "Training Set") %>% 
  bind_rows(
    results_test <- multi_lm %>%
      predict(df_test) %>% 
      as.data.frame() %>%
      mutate(Predicted = df_test$TARGET_WINS, model = "Model 1") %>% 
      bind_rows(mult_lm_final %>% 
                  predict(df_test) %>% 
                  as.data.frame() %>% 
                  mutate(Predicted = df_test$TARGET_WINS, model = "Model 2"),
                mult_lm_final_2 %>% 
                  predict(df_test) %>% as.data.frame() %>% 
                  mutate(Predicted = df_test$TARGET_WINS, model = "Model 3")) %>% 
      rename("Observed" = ".") %>% 
      mutate(dataset = "Testing Set")
  )

# Plot each model with datapoints, model prediction line and R^2
#results %>% 
#  ggplot(mapping = aes(x = Observed, y = Predicted)) +
#  geom_point(pch =21, alpha = 0.3, fill = "skyblue") +
#  geom_smooth(method = "lm", color = "red3") +
#  facet_wrap(dataset~model) +
#  stat_poly_eq(aes(label = paste(..adj.rr.label.., sep = "~~~")), 
#                   label.x.npc = "left", label.y.npc = .9,
#                   formula = y~x, parse = TRUE, size = 3.5)+
#  theme(panel.background = element_blank(),
#        panel.grid = element_blank())

```

The analysis shows the following results:

Model 1 - This model is based on the all predictors on an un-transformed dataset. Results for this model provided an R^2 of about 
of 0.31 on the training set and 0.3 on the testing dataset. 

Model 2 - This model is based on a subset of significant predictors on an un-transformed dataset. Results for this model provided an R^2 of about 
of 0.31 on the training set and 0.31 on the testing dataset. 

Model 3 - This model is based on a subset of significant predictors on an transformed dataset. Results for this model provided an R^2 of about 
of 0.31 on the training set and 0.31 on the testing dataset.

### Model of choice  

Based on these analyses, Model 2 and Model 3 both performed marginally better than Model 1 and could be selected as the linear model of choice when looking at adjusted R2. However, there is greater statistical significance under the third model relative to the others and uses less unnecessary variables to compute our prediction without sacrificing much in terms of adjusted R^2 value. Additionally, Model 3 seems to have lower VIF scores than Models 1 and 2. Because of these factors, as well as its better adjustment for multicollinearity (we noticeably saw less sign-flipping in coefficients), model 3 would be the model of choice. 


## References

- A Modern Approach to Regression with R: Simon Sheather
- Linear Models with R: Julian Faraway. 
- R package vignette, [mixtools: An R Package for Analyzing Finite Mixture Models](https://cran.r-project.org/web/packages/mixtools/vignettes/mixtools.pdf)
- [7 Classic OLS assumptions](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)
- [Detecting Multicolinearity with VIF](https://online.stat.psu.edu/stat462/node/180/)


## Appendix

### A. Crime Dataset Columns

* 

### R Code

```
# =====================================================================================
# Load Libraries and Define Helper functions 
# =====================================================================================

library(MASS)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(forecast)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(mlbench)
library(ggcorrplot)
library(DataExplorer)
library(timeDate)
library(caret)
library(GGally)
library(corrplot)
library(RColorBrewer)
library(tibble)
library(tidyr)
library(tidyverse)
library(dplyr)
library(reshape2)
library(mixtools)
library(tidymodels)
library(ggpmisc)
library(regclass)

#' Print a side-by-side Histogram and QQPlot of Residuals
#'
#' @param model A model
#' @examples
#' residPlot(myModel)
#' @return null
#' @export
residPlot <- function(model) {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
  plot(residuals(model))
  hist(model[["residuals"]], freq = FALSE, breaks = "fd", main = "Residual Histogram",
       xlab = "Residuals",col="lightgreen")
  lines(density(model[["residuals"]], kernel = "ep"),col="blue", lwd=3)
  curve(dnorm(x,mean=mean(model[["residuals"]]), sd=sd(model[["residuals"]])), col="red", lwd=3, lty="dotted", add=T)
  qqnorm(model[["residuals"]], main = "Residual Q-Q plot")
  qqline(model[["residuals"]],col="red", lwd=3, lty="dotted")
  par(mfrow = c(1, 1))
}

#' Print a Variable Importance Plot for the provided model
#'
#' @param model The model
#' @param chart_title The Title to show on the plot
#' @examples
#' variableImportancePlot(myLinearModel, 'My Title)
#' @return null
#' @export
variableImportancePlot <- function(model=NULL, chart_title='Variable Importance Plot') {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  # use caret and gglot to print a variable importance plot
  varImp(model) %>% as.data.frame() %>% 
    ggplot(aes(x = reorder(rownames(.), desc(Overall)), y = Overall)) +
    geom_col(aes(fill = Overall)) +
    theme(panel.background = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    scale_fill_gradient() +
    labs(title = chart_title,
         x = "Parameter",
         y = "Relative Importance")
}


#' Print a Facet Chart of histograms
#'
#' @param df Dataset
#' @param box Facet size (rows)
#' @examples
#' histbox(my_df, 3)
#' @return null
#' @export
histbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]]
    
    for (i in seq_along(ndf)) {
            data <- na.omit(unlist(df[, i]))
            hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
                 xlab = ndf[i], freq = FALSE)
            lines(density(data, kernel = "ep"), col = 'red')
    }
    
    par(mfrow = c(1, 1))
}

#' Extract key performance results from a model
#'
#' @param model A linear model of interest
#' @examples
#' model_performance_extraction(my_model)
#' @return data.frame
#' @export
model_performance_extraction <- function(model=NULL) {
  # Make sure a model was passed
  if (is.null(model)) {
    return
  }
  
  data.frame("RSE" = model$sigma,
             "Adj R2" = model$adj.r.squared,
             "F-Statistic" = model$fstatistic[1])
}

# =====================================================================================
# Load Data set
# =====================================================================================


```
