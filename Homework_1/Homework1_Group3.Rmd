---
title: 'DS 621: Homework 1 (Group3)'
subtitle: 'Homework 1 Description'
author: 'Zach Alexander, Sam Bellows, Donny Lofland, Joshua Registe, Neil Shah, Aaron Zalki'
data: '09/02/2020'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(MASS)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(forecast)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(mlbench)
library(ggcorrplot)
library(DataExplorer)
library(timeDate)
library(caret)
library(GGally)
library(corrplot)
library(RColorBrewer)
library(tibble)
library(tidyr)
library(tidyverse)
library(dplyr)
library(reshape2)
library(mixtools)
```

## Overview

In professional sports, there is a huge interest in attempting to leverage historic statistics to both predict future outcomes (wins/losses) and explore opportunities for tuning or improving a team or individual's performance.  This data-driven approach to sports has gained a large following over the last decade and entered mass media in the form of fantasy leagues, movies (e.g. Moneyball), and websites/podcasts (e.g. FiveThirtyEight).  In this analysis, we will be using a classic baseball data set with the goal to build several different models capable of predicting team wins over a season given on other team stats during that season (i.e. homeruns, strikeouts, base hits, etc).  

We will first explore the data looking for issues or challenges (i.e. missing data, outliers, possible coding errors, multicollinearlity, etc).  Once we have a handle on the data, we will apply any necessary cleaning steps.  Once we have a reasonable dataset to work with, we will build and evaluate three different linear models that predict seasonal wins.  Our dataset includes both training data and evaluation data - we will training using the main training data, then evaluate models based on how well they perform against the holdout evaluation data.  Finally we will select a final model that offers the best compromise for accuracy and simplicity. 


## 1. Data Exploration

*Describe the size and the variables in the moneyball training data set. Consider that too much detail will cause a
manager to lose interest while too little detail will make the manager consider that you arenâ€™t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.*

### Dataset

The moneyball training set contains 17 columns--with one column our target variable "TARGET_WINS" and 2276 rows, covering baseball team performance statistics from the years 1871 to 2006 inclusive. The data has been adjusted to match the performance of a typical 162 game season. The data-set was entirely numerical and contained no categorical variables.

Below, we created a chart that describes each variable in the dataset and the theoretical effect it will have on the number of wins projected for a team.

```{r load_data, echo=FALSE}
df <- read.csv('datasets/moneyball-training-data.csv')
df_eval <- read.csv('datasets/moneyball-evaluation-data.csv')

# Remove superfluous TEAM_ from column names
names(df) <- names(df) %>% str_replace_all('TEAM_', '')
```

![Variables of Interest](./figures/Variables.png)


Given that the Index column had no impact on the target variable, number of wins, it was dropped. 

```{r echo=FALSE}
# Drop the INDEX column - this won't be useful
df <- df %>% dplyr::select(-INDEX)
```


### Summary Stats

We compiled summary statistics on our data set to better understand the data before modeling. 


```{r columns, echo=FALSE}
summary(df)

# gather_df <- df %>% gather(key = 'variable', value = 'value', -TARGET_WINS)
# col_summary <- gather_df %>% group_by(variable) %>% summarise(count = n(), mean = mean(value), med = median(value), sd = sd(value), min = min(value), max = max(value)) %>% as.data.frame()
# kableExtra::kable(col_summary)
```

The first observation is the prevalance of NA's throughout the dataset. On a cursory view, we can quickly see that the average wins per season for a team is about 81, which is exactly half of the total games played in a typical MLB season. Additionally, batters hit, on average, about 9 base hits per game, pitchers throw about 4 base-on-balls (walks) per game, and pitchers record about 5 strikeouts per game, when calculated out over a 162-game season.


### Distributions

Next, we wanted to get an idea of the distribution profiles for each of the variables. 


```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% gather(key = 'variable', value = 'value')

# Histogram plots of each variable
ggplot(gather_df) + 
  geom_histogram(aes(x=value, y = ..density..), bins=30) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=4)
```

The distribution profiles sure the prevalence of kurtosis, specifically right skew in variables BASERUN_CS, BASERUN_CB, FIELDING_E, PITCHING_RB, PITCHING_H and PITCHING_SO. These deviations from a traditional normal distribution can be problematic for linear regression assumptions, and thus we might need to transform the data.  Furthermore BATTING_HBP, BATTING_HR, PITCHING_HR and BATTING_SO appear bimodal. Bimodal features in a dataset are both problematic and interesting and potentially an area of opportunity and exploration.  Bimodal data suggests that there are possibly two different groups or classes of baseball season data.  During those seasons, teams tended to score higher or lower for the bimodal feature.  Two possibilities immediate come to mind.  The bimodal nature could be caused by a rule change such that games before a specific time point had lower values and games after that point had higher values.  Unfortunately, we do not have any features on which year the data is associated.  The second is that teams either do well or not for the given KPI.  Let's consider BATTING_SO (Batters striking out) - if some teams have better players, they might have a reasonable normal distribution lower than those teams with worse players.  We do know that certain teams are able to attract and pay for better players (let's call them Tier 1) and some teams with lower budgets and less visibility (let's call them Tier 2). Even with a given team, they might have "good years" and "off years".  It's possible the distribution of BATTING_SO represents the overlap or superposition of this two distinct curves representing different classes of team.

While we don't tackle feature engineering in this analysis, if we were performing a more in-depth analysis, these are some possible options.

* We have no data linking rows with specific years.  We might attempt to locate additional data sets that link performance to year and leverage change point detection to see if bimodal relationships are linked with all teams at specific points in time.  If change points are present, that suggests something changed affecting all teams (e.g. new rules that improved or lowered the bimodal feature).  We would then create a new categorical feature indicating which class the row data came from, before or after the change point.

* We have no data lining rows with specific teams so cannot directly tell if there are more than one Tier of team.  Instead, we could explore whether there are correlation between the bimodal nature across different bimodal features (i.e. do rows with lower BATTING_HR line up with rows with lower/higher BATTING_SO) within rows. If there are strong correlations between bimodal features, that suggests there might be two classes of teams (or yearly team performance) and we could create a new categorical variable(s) indicating the probable Tier per row.  Alternatively, we could assign a new numerical feature indicating the probability a row fell into which class.

* R provides a package, `mixtools` (see R Vignette) which helps regress *mixed models* where data can be subdivided into subgroups.  Here is a quick example showing a possible mix within BATTING_SO:

```{r, echo=FALSE}
# Select BATTING_SO column and remove any missing data
df_mix <- df %>% 
  dplyr::select(BATTING_SO) %>%
  drop_na()

# Calculate mixed distributions for BATTING_SO
batting_so_mix <- normalmixEM(df_mix$BATTING_SO, 
                              lambda = .5, 
                              mu = c(400, 1200), 
                              sigma = 5, 
                              maxit=60)

# Simple plot to illustrate possible bimodal mix of groups
plot(batting_so_mix, 
     whichplots = 2,
     density=TRUE, 
     main2="BATTING_SO Possible Distributions", 
     xlab2="BATTING_SO")
```

### Boxplots

We elected to use box-plots to get an idea of the spread of each variable. 

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% gather(key = 'variable', value = 'value')

# Boxplots for each variable
ggplot(gather_df, aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(. ~variable, scales='free', ncol=6)
```

The box-plots reveal significant outliers and uniform (zero-only) values for many of our predictor variables. Outliers will need to imputed if necessary, and sparse data-sets might need to be dropped. 

### Variable Plots

Finally, we wanted to plot scatter plots of each variable versus the target variable, TARGET_WINS, to get an idea of the relationship between them. 

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
featurePlot(df[,2:ncol(df)], df[,1], pch = 20)
```

The plots indicate some clear relationships, such as hitting more doubles or more home runs clearly improves the number of wins.


Overall our collection of plots also reveal significant issues with the data. 
Most of the predictor variables are skewed or non-normally distributed, and wlll need to be transformed. There are many data points that contain missing data that will need to be either imputed or discarded. Finally it appears we have some missing data encoded as 0 and some nonsensical outliers.

There is a team with 0 wins in the dataset. This seems unlikely. Many of the hitting categories include teams at 0; it is unlikely that a team hit 0 home runs over the course of a season.

The pitching variables also include many 0's, for instance there are multiple teams with 0 strikeouts by their pitchers over the season which is extremely unlikely. The pitching data also includes strange outliers such as a team logging 20,000 strikeouts, that would be an average of 160 strikeouts per game which is impossible. Also team pitching walks and team pitching hits have strange outliers.

Lastly, the error variable makes little sense. From my experience watching baseball, teams usually score 2 or less errors per game, which would lead to an overall team error of approximately 320 over the course of a season, which does not match the scale of the error variable.

### Missing Data

When we initially viewed the first few rows of the raw data, we already noticed missing data.  Let's assess which fields have missing data.

```{r echo=FALSE} 
missing <- colSums(df %>% sapply(is.na))
missing_pct <- round(missing / nrow(df) * 100, 2)
stack(sort(missing_pct, decreasing = TRUE))
```

Notice that ~91.6% of the rows are missing the BATTING_HBP field - we will just drop this column from consideration.  The columns BASERUN_CS (base run caught stealing) and BASERUN_SB (stolen bases) both have missing values.  According to baseball history, stolen bases weren't tracked officially until 1887, so some of the missing data could be from 1871-1886.  We will impute those value.  There are a high percentage of missing BATTING_SO (batter strike outs) and PITCHING_SO (pitching strike outs) which seem highly unlikely - we will also impute those missing values.

```{r echo=FALSE}
# Drop the BATTING_HBP field
df <- df %>% select(-BATTING_HBP)

# We have chosen to impute the median as there are strong outliers that may skew the mean. Could revisit for advanced imputation via prediction later.
no_outlier_df <- df

# 4000 strikeouts is an average of 25 strikeouts per game, which is ridiculous.
no_outlier_df$PITCHING_SO <- ifelse(no_outlier_df$PITCHING_SO > 4000, NA, no_outlier_df$PITCHING_SO)

# 5000 hits is an average of 30 hits allowed per game, which is also ridiculous.
no_outlier_df$PITCHING_H <- ifelse(no_outlier_df$PITCHING_H > 5000, NA, no_outlier_df$PITCHING_H)

# 2000 walks is an average of 13 walks per game which is unlikely.
no_outlier_df$PITCHING_BB <- ifelse(no_outlier_df$PITCHING_BB > 2000, NA, no_outlier_df$PITCHING_BB)

# more than 480 errors is an average of 3 per game which is unlikely.
no_outlier_df$FIELDING_E <- ifelse(no_outlier_df$FIELDING_E > 480, NA, no_outlier_df$FIELDING_E)

clean_df <- no_outlier_df %>% impute(what = 'median') %>% as.data.frame()

# From the plots, the team with 0 wins has 0 in multiple other categories. I believe this is missing data, not valid data.
clean_df <- clean_df %>% filter(TARGET_WINS != 0)

gather_clean_df <- clean_df %>% gather(key = 'variable', value = 'value', -TARGET_WINS)
```

### Feature-Target Correlations

Building off the scatter plots, we wanted to quantify the correlations between our target variable and predictor variable. We will want to choose those with stronger positive or negative correlations.  Features with correlations closer to zero will probably not provide any meaningful information on explaining wins by a team.

```{r echo=FALSE}
stack(sort(cor(clean_df[,1], clean_df[,2:ncol(clean_df)])[,], decreasing=TRUE))
```

BATTING_H and BATTING_2B have the highest correlation (positive) with TOTAL_WINs; this makes sense given that more hits means more points/runs, and thus liklihood to win the game. The other variables have weak or slightly negative corelation, which can imply they might have little predictive power.

### Multicolinearlity

One problem that can occur with multi-variable regression is correlation between variables, or multicolinearity. A quick check is to run correlations between variables.   

```{r echo=FALSE}
correlation = cor(clean_df, use = 'pairwise.complete.obs')

corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))
```

When we start considering features for our models, we'll need to account for the correlations between features and avoid including pairs with strong correlations.

## 2. Data Preparation

### Removed Fields

We removed the BATTING_HBP field as it was missing >90% of the data and the INDEX field as it offers no information for a model.  

### Missing Values

Missing values found in BASERUN_CS (Caught Stolen Bases), FIELDING_DP (Double plays), BASERUN_SB (Stolen Bases), BATTING_SO (Batter Strike outs), PITCHING_SO (Pitcher Strike outs) were all replaced with median values.  It is highly unlikely that teams had none of these during an entire season.

### Outliers

There are unreasonable outliers found in PITCHING_SO (pitching strikeouts), PITCHIN_H (allowed hits per game), PITCHING_BB (walks), and  FIELDING_E (fielding errors) that exceed what is reasonable or possible given standard game length.  While specific games might have outliers (e.g. in a game with extra innings), we wouldn't expect the totals per season to allow for outliers in every game.  Given this, we will replace any outliers with the median for the data set. Limits we set included: > 4000 PITCHING_SO (25 striekouts per game), > 5000 PITCHING_H (30 hits allowed per game), > 2000 PITCHING_BB (13 walks per game) and > 480 FIELDING_E (3 errors per game).

### Transform non-normal variables

From our plots above, we can see that some of our variables are highly skewed. To address this, we decided to perform some tranformations to make them more normally distributed. Here are some plots to demonstrate the changes in distributions before and after the transformations:  

```{r echo=FALSE, fig.height=12, fig.width=10, message=FALSE, warning=FALSE}
# TODO - Transform skewed data here.  Need to save off any parameters for applying against evaluation dataset.


# created empty data frame to store transformed variables
df_temp <- data.frame(matrix(ncol = 1, nrow = length(clean_df$TARGET_WINS)))

# performed boxcox transformation after identifying proper lambda
df_temp$BATTING_3B <- clean_df$BATTING_3B
batting3b_lambda <- BoxCox.lambda(clean_df$BATTING_3B)
df_temp$BATTING_3B_transform <- log(clean_df$BATTING_3B)

# performed boxcox transformation after identifying proper lambda
df_temp$BATTING_HR <- clean_df$BATTING_HR
battingHR_lambda <- BoxCox.lambda(clean_df$BATTING_HR)
df_temp$BATTING_HR_transform <- BoxCox(clean_df$BATTING_HR, battingHR_lambda)

# performed a log transformation
df_temp$PITCHING_BB <- clean_df$PITCHING_BB
df_temp$PITCHING_BB_transform <- log(clean_df$PITCHING_BB)

# performed a log transformation
df_temp$PITCHING_SO <- clean_df$PITCHING_SO
df_temp$PITCHING_SO_transform <- log(clean_df$PITCHING_SO)

# performed an inverse log transformation
df_temp$FIELDING_E <- clean_df$FIELDING_E
df_temp$FIELDING_E_transform <- 1/log(clean_df$FIELDING_E)

# performed a log transformation
df_temp$BASERUN_SB <- clean_df$BASERUN_SB
df_temp$BASERUN_SB_transform <- log(clean_df$BASERUN_SB)

df_temp <- df_temp[, 2:13]

histbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]]
    for (i in seq_along(ndf)) {
            data <- na.omit(unlist(df[, i]))
            hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
                 xlab = ndf[i], freq = FALSE)
            lines(density(data, kernel = "ep"), col = 'red')
    }
    par(mfrow = c(1, 1))
}

histbox(df_temp, c(6, 2))
```


## 3. Build Models

Using the training data set, build at least three different multiple linear regression models, using different variables
(or the same variables with different transformations). Since we have not yet covered automated variable
selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise
selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model,
indicate why this was done.

Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it
would be reasonably expected that such a team would win more games. However, if the coefficient is negative
(suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model
even though it is counter intuitive? Why? The boss needs to know.

```{r echo=FALSE}
#' Print a side-by-side Histogram and QQPlot of Residuals
#'
#' @param model A model
#' @examples
#' residPlot(myModel)
#' @return null
#' @export
residPlot <- function(model) {
  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
  plot(residuals(model))
  hist(model[["residuals"]], freq = FALSE, breaks = "fd", main = "Residual Histogram",
       xlab = "Residuals",col="lightgreen")
  lines(density(model[["residuals"]], kernel = "ep"),col="blue", lwd=3)
  curve(dnorm(x,mean=mean(model[["residuals"]]), sd=sd(model[["residuals"]])), col="red", lwd=3, lty="dotted", add=T)
  qqnorm(model[["residuals"]], main = "Residual Q-Q plot")
  qqline(model[["residuals"]],col="red", lwd=3, lty="dotted")
  par(mfrow = c(1, 1))

}
```

### Model 1
```{r echo=FALSE}
multi_lm <- lm(TARGET_WINS ~ ., clean_df)
(lm_s <- summary(multi_lm))
confint(multi_lm)
residPlot(lm_s)

mult_lm_final <- stepAIC(multi_lm, direction = "both",
                         scope = list(upper = multi_lm, lower = ~ 1),
                         scale = 0, trace = FALSE)
(lmf_s <- summary(mult_lm_final))
residPlot(lmf_s)
```

We can use a simple decision tree to help visualize feature importance.

```{r echo=FALSE}
dt_m <- caret::train(TARGET_WINS ~ ., data = clean_df, tuneLength = 19L, method = 'rpart2')
rpart.plot(dt_m$finalModel, type = 1L)
```

We can also explicitly call out our variable importance using the Caret package which will determine our feature importance for each of the multiple linear regression models individually.

```{r echo=FALSE}

VarImpPlot<-function(LINEARMODEL,TITLE){
  varImp(LINEARMODEL) %>% as.data.frame() %>% 
    ggplot(aes(x = reorder(rownames(.),desc(Overall)), y = Overall))+
    geom_col(aes(fill = Overall))+
    theme(panel.background = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90))+
    scale_fill_gradient()+
    labs(title = TITLE,
         x = "Parameter",
         y = "Relative Importance")
}



```


```{r echo=FALSE}
VarImpPlot(mult_lm_final, "Model 1 LM Variable Importance")
```


### Model 2

In our second model, we decided to utilize some of our transformed variables to compare against our initial model. As a result of the transformations, there were just a few values that needed to be imputed -- we imputed using the mean value for the given variable.
```{r echo=FALSE}
clean_trans_df <- data.frame(cbind(TARGET_WINS = clean_df$TARGET_WINS, 
                        BATTING_H = clean_df$BATTING_H,
                        BATTING_2B = clean_df$BATTING_2B,
                        BATTING_3B_transform = df_temp$BATTING_3B_transform,
                        BATTING_HR_transform = df_temp$BATTING_HR_transform,
                        BATTING_BB = clean_df$BATTING_BB,
                        BATTING_SO = clean_df$BATTING_SO,
                        BASERUN_SB_transform = df_temp$BASERUN_SB_transform,
                        BASERUN_CS = clean_df$BASERUN_CS,
                        PITCHING_H = clean_df$PITCHING_H,
                        PITCHING_HR = clean_df$PITCHING_HR,
                        PITCHING_BB_transform = df_temp$PITCHING_BB_transform,
                        PITCHING_SO_transform = df_temp$PITCHING_SO_transform,
                        FIELDING_E_transform = df_temp$FIELDING_E_transform,
                        FIELDING_DP = clean_df$FIELDING_DP))

is.na(clean_trans_df)<-sapply(clean_trans_df, is.infinite)

mean = mean(clean_trans_df$BATTING_3B_transform, na.rm = TRUE)
mean2 = mean(clean_trans_df$BASERUN_SB_transform, na.rm = TRUE)
mean3 = mean(clean_trans_df$PITCHING_SO_transform, na.rm = TRUE)

clean_trans_df$BATTING_3B_transform[is.na(clean_trans_df$BATTING_3B_transform)] <- mean
clean_trans_df$BASERUN_SB_transform[is.na(clean_trans_df$BASERUN_SB_transform)] <- mean2
clean_trans_df$PITCHING_SO_transform[is.na(clean_trans_df$PITCHING_SO_transform)] <- mean3
```

Then, similar to model 1, we used Stepwise selection to determine feature importance and select the simplest model possible.
```{r echo=FALSE}
multi_lm_2 <- lm(TARGET_WINS ~ ., clean_trans_df)
(lm_s_2 <- summary(multi_lm_2))
confint(multi_lm_2)
residPlot(lm_s_2)

mult_lm_final_2 <- stepAIC(multi_lm_2, direction = "both",
                         scope = list(upper = multi_lm_2, lower = ~ 1),
                         scale = 0, trace = FALSE)
(lmf_s_2 <- summary(mult_lm_final_2))
residPlot(lmf_s_2)
```

We can assess variable importance of our model with our original dataset (model 1) to our transformed dataset (model 2) as shown below. We notice that "Batting_BB" and "Batting_H" are much closer together in terms of weight in predicting target_wins than in model 1 (pre-transformed)

```{r echo=FALSE}
VarImpPlot(mult_lm_final_2, "Model 2 LM Variable Importance")
```

## 4. Select Models

For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to
explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other
relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a)
mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

## References

- A Modern Approach to Regression with R: Simon Sheather
- Linear Models with R: Julian Faraway. 
- R package vignette, [mixtools: An R Package for Analyzing Finite Mixture Models](https://cran.r-project.org/web/packages/mixtools/vignettes/mixtools.pdf)

## Appendix

### A. Moneyball Dataset Columns

* INDEX: Identification Variable(Do not use)
* TARGET_WINS: Number of wins
* TEAM_BATTING_H : Base Hits by batters (1B,2B,3B,HR)
* TEAM_BATTING_2B: Doubles by batters (2B)
* TEAM_BATTING_3B: Triples by batters (3B)
* TEAM_BATTING_HR: Home runs by batters (4B)
* TEAM_BATTING_BB: Walks by batters
* TEAM_BATTING_HBP: Batters hit by pitch (get a free base)
* TEAM_BATTING_SO: Strikeouts by batters
* TEAM_BASERUN_SB: Stolen bases
* TEAM_BASERUN_CS: Caught stealing
* TEAM_FIELDING_E: Errors
* TEAM_FIELDING_DP: Double Plays
* TEAM_PITCHING_BB: Walks allowed
* TEAM_PITCHING_H: Hits allowed
* TEAM_PITCHING_HR: Homeruns allowed
* TEAM_PITCHING_SO: Strikeouts by pitchers

### R Code



```{r appendix, echo=TRUE}

# Show all code here

```

