---
title: 'DS 621: Homwork 1 (Group3)'
subtitle: 'Homework 1 Description'
author: 'Zach Alexander, Sam Bellows, Donny Lofland, Joshua Registe, Neil Shah, Aaron Zalki'
data: '09/02/2020'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(MASS)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(forecast)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(mlbench)
library(ggcorrplot)
library(DataExplorer)
library(timeDate)
library(caret)
library(GGally)
library(corrplot)
library(RColorBrewer)
library(tibble)
library(tidyr)
library(tidyverse)
library(dplyr)
library(reshape2)
```

## Overview

In professional sports, there is a huge interest in attempting to leverage historic statistics to both predict future outcomes (wins/losses) and explore opportunities for tuning or improving a team or individual's performance.  This data-driven approach to sports has gained a large following over the last decade and entered mass media in the form of fantasy leagues, movies (e.g. Moneyball), and websites/podcasts (e.g. FiveThirtyEight).  In this analysis, we will be using a classic baseball data set with the goal to build several different models capable of predicting team wins over a season given on other team stats during that season (i.e. homeruns, strikeouts, base hits, etc).  The data set containing approximately 2200 records, representing professional baseball team statistics from the years 1871 to 2006 inclusive and each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.  

We will first explore the data looking for issues or challenges (i.e. missing data, outliers, possible coding errors, multicollinearlity, etc).  Once we have a handle on the data, we will apply any necessary cleaning steps.  Once we have a reasonable dataset to work with, we will build and evaluate three different linear models that predict seasonal wins.  Our dataset includes both training data and evaluation data - we will training using the main training data, then evaluate models based on how well they perform against the holdout evaluation data.  Finally we will select a final model that offers the best compromise for accuracy and simplicity. 


## 1. Data Exploration

*Describe the size and the variables in the moneyball training data set. Consider that too much detail will cause a
manager to lose interest while too little detail will make the manager consider that you arenâ€™t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.*

### Dataset

#### Columns

Below, we created a chart that describes each variable in the dataset and the theoretical effect it will have on the number of wins projected for a team.

```{r load_data, echo=FALSE}
df <- read.csv('datasets/moneyball-training-data.csv')
df_eval <- read.csv('datasets/moneyball-evaluation-data.csv')

# Remove superfluous TEAM_ from column names
names(df) <- names(df) %>% str_replace_all('TEAM_', '')
```

![Variables of Interest](./figures/Variables.png)

With the variables defined, we thought it would be helpful to look at the first few rows to get a sense for the data.

```{r data_head, echo=FALSE} 
# Display firt few row of raw data set
kable(df[1:5,]) %>% 
  kable_styling(bootstrap_options = "basic")
```

We also decided to drop the INDEX column as it offers no information towards predictive models.

```{r echo=FALSE}
# Drop the INDEX column - this won't be useful
df <- df %>% dplyr::select(-INDEX)
```

#### Dimensions

```{r echo=FALSE}
# Display row and column counts
c(nrow(df), ncol(df))
```
With the INDEX column dropped, we can see that there are 2276 total rows, and 16 columns/variables in our dataset.  

#### Summary Stats

Before showing visual plots of each variable, we thought it would be helpful to first generate a summary that provides basic descriptive statistics for each column.

```{r columns, echo=FALSE}
summary(df)

# gather_df <- df %>% gather(key = 'variable', value = 'value', -TARGET_WINS)
# col_summary <- gather_df %>% group_by(variable) %>% summarise(count = n(), mean = mean(value), med = median(value), sd = sd(value), min = min(value), max = max(value)) %>% as.data.frame()
# kableExtra::kable(col_summary)
```
We will go into more detail later, but we can quickly see that the average wins per season for a team is about 81, which is exactly half of the total games played in a typical MLB season. Additionally, batters hit, on average, about 9 base hits per game, pitchers throw about 4 base-on-balls (walks) per game, and pitchers record about 5 strikeouts per game, when calculated out over a 162-game season.

#### Distributions

```{r echo=FALSE, fig.height=12, fig.width=10}

histbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]] # Thanks for nothing, scale!!
    for (i in seq_along(ndf)) {
        data <- na.omit(unlist(df[, i]))
        hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
             xlab = ndf[i], freq = FALSE)
        lines(density(data, kernel = "ep"), col = 'blue')
    }
    par(mfrow = c(1, 1))
}

histbox(df, c(4, 4))
```

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% gather(key = 'variable', value = 'value')

# Histogram plots of each variable
ggplot(gather_df) + 
  geom_histogram(aes(x=value, y = ..density..), bins=30) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=4)

# Boxplots for each variable
ggplot(gather_df, aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(. ~variable, scales='free', ncol=6)
```

Looking at both the histograms and boxplots, we notice that many of the features are NOT normally distributed (a concern for linear regressions), there are a significant number of outliers that we will have to consider, and we see a large number of zero values for some features.  For zero values and outliers, we will want to qualitatively assess whether those make sense and whether we need to transform them in some way.

#### Variable Plots

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
featurePlot(df[,2:ncol(df)], df[,1], pch = 20)
```

Here we plot every variable against the target variable number of wins to get a feeling for which variables may be predictive of wins. The plots indicate some clear relationships, such as hitting more doubles or more home runs clearly improves the number of wins.

The plots also reveal significant issues with the data. First, there are many data points that contain missing data that will need to be either imputed or discarded. Second, it appears we have some missing data encoded as 0 and some nonsensical outliers.

There is a team with 0 wins in the dataset. This seems unlikely. Many of the hitting categories include teams at 0; it is unlikely that a team hit 0 home runs over the course of a season.

The pitching variables also include many 0's, for instance there are multiple teams with 0 strikeouts by their pitchers over the season which is extremely unlikely. The pitching data also includes strange outliers such as a team logging 20,000 strikeouts, that would be an average of 160 strikeouts per game which is impossible. Also team pitching walks and team pitching hits have strange outliers.

Lastly, the error variable makes no sense to me. From my experience watching baseball, teams usually score 2 or less errors per game, which would lead to an overall team error of approximately 320 over the course of a season, which does not match the scale of the error variable.

#### Missing Data

When we initially viewed the first few rows of the raw data, we already noticed missing data.  Let's assess which fields have missing data.

```{r echo=FALSE} 
missing <- colSums(df %>% sapply(is.na))
missing_pct <- round(missing / nrow(df) * 100, 2)
stack(sort(missing_pct, decreasing = TRUE))
```

Notice that ~91.6% of the rows are missing the BATTING_HBP field - we will just drop this column from consideration.  The columns BASERUN_CS (base run caught stealing) and BASERUN_SB (stolen bases) both have missing values.  According to baseball history, stolen bases weren't tracked officially until 1887, so some of the missing data could be from 1871-1886.  We will impute those value.  There are a high percentage of missing BATTING_SO (batter strike outs) and PITCHING_SO (pitching strike outs) which seem highly unlikely - we will also impute those missing values.

```{r echo=FALSE}
# Drop the BATTING_HBP field
df <- df %>% select(-BATTING_HBP)

# We have chosen to impute the median as there are strong outliers that may skew the mean. Could revisit for advanced imputation via prediction later.
no_outlier_df <- df

# 4000 strikeouts is an average of 25 strikeouts per game, which is ridiculous.
no_outlier_df$PITCHING_SO <- ifelse(no_outlier_df$PITCHING_SO > 4000, NA, no_outlier_df$PITCHING_SO)

# 5000 hits is an average of 30 hits allowed per game, which is also ridiculous.
no_outlier_df$PITCHING_H <- ifelse(no_outlier_df$PITCHING_H > 5000, NA, no_outlier_df$PITCHING_H)

# 2000 walks is an average of 13 walks per game which is unlikely.
no_outlier_df$PITCHING_BB <- ifelse(no_outlier_df$PITCHING_BB > 2000, NA, no_outlier_df$PITCHING_BB)

# more than 480 errors is an average of 3 per game which is unlikely.
no_outlier_df$FIELDING_E <- ifelse(no_outlier_df$FIELDING_E > 480, NA, no_outlier_df$FIELDING_E)

clean_df <- no_outlier_df %>% impute(what = 'median') %>% as.data.frame()

# From the plots, the team with 0 wins has 0 in multiple other categories. I believe this is missing data, not valid data.
clean_df <- clean_df %>% filter(TARGET_WINS != 0)

gather_clean_df <- clean_df %>% gather(key = 'variable', value = 'value', -TARGET_WINS)
```

#### Feature-Target Correlations

```{r echo=FALSE}
stack(sort(cor(clean_df[,1], clean_df[,2:ncol(clean_df)])[,], decreasing=TRUE))
```

When evaluating features for including in models, we will want to choose those with stronger positive or negative correlations.  Features with correlations closer to zero will probably not provide any meaningful information on explaining wins by a team.

#### Pairplot

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
clean_df %>% ggpairs(columns = 2:ncol(clean_df), progress = FALSE)
```

#### Multicolinearlity

```{r echo=FALSE}
correlation = cor(clean_df, use = 'pairwise.complete.obs')

corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))

palette = colorRampPalette(c("green", "white", "red"))(20)
heatmap(x = correlation, col = palette, symm = T)
```

When we start considering features for our models, we'll need to account for the correlations between features and avoid including pairs with strong correlations.

## 2. Data Preparation

### Removed Fields

We removed the BATTING_HBP field as it was missing >90% of the data and the INDEX field as it offers no information for a model.  

### Missing Values

Missing values found in BASERUN_CS (Caught Stolen Bases), FIELDING_DP (Double plays), BASERUN_SB (Stolen Bases), BATTING_SO (Batter Strike outs), PITCHING_SO (Pitcher Strike outs) were all replaced with median values.  It is highly unlikely that teams had none of these during an entire season.

### Outliers

There are unreasonable outliers found in PITCHING_SO (pitching strikeouts), PITCHIN_H (allowed hits per game), PITCHING_BB (walks), and  FIELDING_E (fielding errors) that exceed what is reasonable or possible given standard game length.  While specific games might have outliers (e.g. in a game with extra innings), we wouldn't expect the totals per season to allow for outliers in every game.  Given this, we will replace any outliers with the median for the data set. Limits we set included: > 4000 PITCHING_SO (25 striekouts per game), > 5000 PITCHING_H (30 hits allowed per game), > 2000 PITCHING_BB (13 walks per game) and > 480 FIELDING_E (3 errors per game).

### Transform non-normal variables

From our plots above, we can see that some of our variables are highly skewed. To address this, we decided to perform some tranformations to make them more normally distributed. Here are some plots to demonstrate the changes in distributions before and after the transformations:  

```{r echo=FALSE, fig.height=12, fig.width=10, message=FALSE, warning=FALSE}
# TODO - Transform skewed data here.  Need to save off any parameters for applying against evaluation dataset.


# created empty data frame to store transformed variables
df_temp <- data.frame(matrix(ncol = 1, nrow = length(clean_df$TARGET_WINS)))

# performed boxcox transformation after identifying proper lambda
df_temp$BATTING_3B <- clean_df$BATTING_3B
batting3b_lambda <- BoxCox.lambda(clean_df$BATTING_3B)
df_temp$BATTING_3B_transform <- log(clean_df$BATTING_3B)

# performed boxcox transformation after identifying proper lambda
df_temp$BATTING_HR <- clean_df$BATTING_HR
battingHR_lambda <- BoxCox.lambda(clean_df$BATTING_HR)
df_temp$BATTING_HR_transform <- BoxCox(clean_df$BATTING_HR, battingHR_lambda)

# performed a log transformation
df_temp$PITCHING_BB <- clean_df$PITCHING_BB
df_temp$PITCHING_BB_transform <- log(clean_df$PITCHING_BB)

# performed a log transformation
df_temp$PITCHING_SO <- clean_df$PITCHING_SO
df_temp$PITCHING_SO_transform <- log(clean_df$PITCHING_SO)

# performed an inverse log transformation
df_temp$FIELDING_E <- clean_df$FIELDING_E
df_temp$FIELDING_E_transform <- 1/log(clean_df$FIELDING_E)

# performed a log transformation
df_temp$BASERUN_SB <- clean_df$BASERUN_SB
df_temp$BASERUN_SB_transform <- log(clean_df$BASERUN_SB)

df_temp <- df_temp[, 2:13]

histbox <- function(df, box) {
    par(mfrow = box)
    ndf <- dimnames(df)[[2]]
    for (i in seq_along(ndf)) {
            data <- na.omit(unlist(df[, i]))
            hist(data, breaks = "fd", main = paste("Histogram of", ndf[i]),
                 xlab = ndf[i], freq = FALSE)
            lines(density(data, kernel = "ep"), col = 'red')
    }
    par(mfrow = c(1, 1))
}

histbox(df_temp, c(6, 2))
```


## 3. Build Models

Using the training data set, build at least three different multiple linear regression models, using different variables
(or the same variables with different transformations). Since we have not yet covered automated variable
selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise
selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model,
indicate why this was done.

Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it
would be reasonably expected that such a team would win more games. However, if the coefficient is negative
(suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model
even though it is counter intuitive? Why? The boss needs to know.

```{r echo=FALSE}
#' Print a side-by-side Histogram and QQPlot of Residuals
#'
#' @param model A model
#' @examples
#' residPlot(myModel)
#' @return null
#' @export
residPlot <- function(model) {
  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
  plot(residuals(model))
  hist(model[["residuals"]], freq = FALSE, breaks = "fd", main = "Residual Histogram",
       xlab = "Residuals",col="lightgreen")
  lines(density(model[["residuals"]], kernel = "ep"),col="blue", lwd=3)
  curve(dnorm(x,mean=mean(model[["residuals"]]), sd=sd(model[["residuals"]])), col="red", lwd=3, lty="dotted", add=T)
  qqnorm(model[["residuals"]], main = "Residual Q-Q plot")
  qqline(model[["residuals"]],col="red", lwd=3, lty="dotted")
  par(mfrow = c(1, 1))

}
```

### Model 1
```{r echo=FALSE}
multi_lm <- lm(TARGET_WINS ~ ., clean_df)
(lm_s <- summary(multi_lm))
confint(multi_lm)
residPlot(lm_s)

mult_lm_final <- stepAIC(multi_lm, direction = "both",
                         scope = list(upper = multi_lm, lower = ~ 1),
                         scale = 0, trace = FALSE)
(lmf_s <- summary(mult_lm_final))
residPlot(lmf_s)
```

We can use a simple decision tree to help visualize feature importance.

```{r echo=FALSE}
dt_m <- caret::train(TARGET_WINS ~ ., data = clean_df, tuneLength = 19L, method = 'rpart2')
rpart.plot(dt_m$finalModel, type = 1L)
```

We can also explicitly call out our variable importance using the Caret package which will determine our feature importance for each of the multiple linear regression models individually.

```{r echo=FALSE}

VarImpPlot<-function(LINEARMODEL,TITLE){
  varImp(LINEARMODEL) %>% as.data.frame() %>% 
    ggplot(aes(x = reorder(rownames(.),desc(Overall)), y = Overall))+
    geom_col(aes(fill = Overall))+
    theme(panel.background = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90))+
    scale_fill_gradient()+
    labs(title = TITLE,
         x = "Parameter",
         y = "Relative Importance")
}



```


```{r echo=FALSE}
VarImpPlot(mult_lm_final, "Model 1 LM Variable Importance")
```


### Model 2

In our second model, we decided to utilize some of our transformed variables to compare against our initial model. As a result of the transformations, there were just a few values that needed to be imputed -- we imputed using the mean value for the given variable.
```{r echo=FALSE}
clean_trans_df <- data.frame(cbind(TARGET_WINS = clean_df$TARGET_WINS, 
                        BATTING_H = clean_df$BATTING_H,
                        BATTING_2B = clean_df$BATTING_2B,
                        BATTING_3B_transform = df_temp$BATTING_3B_transform,
                        BATTING_HR_transform = df_temp$BATTING_HR_transform,
                        BATTING_BB = clean_df$BATTING_BB,
                        BATTING_SO = clean_df$BATTING_SO,
                        BASERUN_SB_transform = df_temp$BASERUN_SB_transform,
                        BASERUN_CS = clean_df$BASERUN_CS,
                        PITCHING_H = clean_df$PITCHING_H,
                        PITCHING_HR = clean_df$PITCHING_HR,
                        PITCHING_BB_transform = df_temp$PITCHING_BB_transform,
                        PITCHING_SO_transform = df_temp$PITCHING_SO_transform,
                        FIELDING_E_transform = df_temp$FIELDING_E_transform,
                        FIELDING_DP = clean_df$FIELDING_DP))

is.na(clean_trans_df)<-sapply(clean_trans_df, is.infinite)

mean = mean(clean_trans_df$BATTING_3B_transform, na.rm = TRUE)
mean2 = mean(clean_trans_df$BASERUN_SB_transform, na.rm = TRUE)
mean3 = mean(clean_trans_df$PITCHING_SO_transform, na.rm = TRUE)

clean_trans_df$BATTING_3B_transform[is.na(clean_trans_df$BATTING_3B_transform)] <- mean
clean_trans_df$BASERUN_SB_transform[is.na(clean_trans_df$BASERUN_SB_transform)] <- mean2
clean_trans_df$PITCHING_SO_transform[is.na(clean_trans_df$PITCHING_SO_transform)] <- mean3
```

Then, similar to model 1, we used Stepwise selection to determine feature importance and select the simplest model possible.
```{r echo=FALSE}
multi_lm_2 <- lm(TARGET_WINS ~ ., clean_trans_df)
(lm_s_2 <- summary(multi_lm_2))
confint(multi_lm_2)
residPlot(lm_s_2)

mult_lm_final_2 <- stepAIC(multi_lm_2, direction = "both",
                         scope = list(upper = multi_lm_2, lower = ~ 1),
                         scale = 0, trace = FALSE)
(lmf_s_2 <- summary(mult_lm_final_2))
residPlot(lmf_s_2)
```

We can assess variable importance of our model with our original dataset (model 1) to our transformed dataset (model 2) as shown below. We notice that "Batting_BB" and "Batting_H" are much closer together in terms of weight in predicting target_wins than in model 1 (pre-transformed)

```{r echo=FALSE}
VarImpPlot(mult_lm_final_2, "Model 2 LM Variable Importance")
```

## 4. Select Models

For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to
explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other
relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a)
mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

## References

- A
- B

## Appendix

### A. Moneyball Dataset Columns

* INDEX: Identification Variable(Do not use)
* TARGET_WINS: Number of wins
* TEAM_BATTING_H : Base Hits by batters (1B,2B,3B,HR)
* TEAM_BATTING_2B: Doubles by batters (2B)
* TEAM_BATTING_3B: Triples by batters (3B)
* TEAM_BATTING_HR: Home runs by batters (4B)
* TEAM_BATTING_BB: Walks by batters
* TEAM_BATTING_HBP: Batters hit by pitch (get a free base)
* TEAM_BATTING_SO: Strikeouts by batters
* TEAM_BASERUN_SB: Stolen bases
* TEAM_BASERUN_CS: Caught stealing
* TEAM_FIELDING_E: Errors
* TEAM_FIELDING_DP: Double Plays
* TEAM_PITCHING_BB: Walks allowed
* TEAM_PITCHING_H: Hits allowed
* TEAM_PITCHING_HR: Homeruns allowed
* TEAM_PITCHING_SO: Strikeouts by pitchers

### R Code

```{r appendix, echo=TRUE}

# Show all code here

```

